
SPEC

Right now, each traced function call puts logged information into memory. How could we add support for logging where we actually send the information to the endpoint immediately at the end of the step? We'd need to send enough data so that the ordering/lineage/inheritance (structure) of the OpenTelemetry span data is kept intact, of couse. 

PLAN


```python
POST /v1/uploads/stream
- Accepts a single Event object (which includes its compute steps)
- Returns 200 OK or error
```

And a simpler client:

```python:synth_sdk/tracing/immediate_client.py
import os
import requests
import logging
from typing import Dict
from synth_sdk.tracing.abstractions import Event
from synth_sdk.tracing.config import TracingConfig, LoggingMode

logger = logging.getLogger(__name__)

class ImmediateLogClient:
    def __init__(self, config: TracingConfig):
        self.config = config
        self.session = requests.Session()
        self._setup_auth()
    
    def _setup_auth(self):
        self.session.headers["Authorization"] = f"Bearer {self.config.api_key}"
    
    def send_event(self, event: Event, system_info: Dict[str, str]) -> bool:
        """
        Sends a single event with its compute steps to the streaming endpoint.
        
        Args:
            event: The Event object containing compute steps
            system_info: Dict containing system_name, system_id, and system_instance_id
        """
        url = f"{self.config.base_url}/v1/uploads/stream"
        payload = {
            "event": event.to_dict(),
            "system_info": system_info
        }
        
        try:
            response = self.session.post(url, json=payload)
            response.raise_for_status()
            return True
        except Exception as e:
            logger.error(f"Failed to stream event: {e}")
            # Still store in event_store as backup
            from synth_sdk.tracing.events.store import event_store
            event_store.add_event(
                system_info["system_name"],
                system_info["system_id"], 
                system_info["system_instance_id"],
                event
            )
            return False
```

Then in the decorator, we only need to add immediate logging when we're storing an event (which already has its compute steps):

```python:synth_sdk/tracing/decorators.py
# In trace_system_sync and trace_system_async decorators
# Where we store events:

if hasattr(_local, "system_instance_id"):
    # Store event in event_store
    event_store.add_event(
        _local.system_name,
        _local.system_id,
        _local.system_instance_id,
        current_event,
    )
    
    # If immediate logging is enabled, also stream the event
    if logging_mode == LoggingMode.INSTANT:
        system_info = {
            "system_name": _local.system_name,
            "system_id": _local.system_id,
            "system_instance_id": _local.system_instance_id
        }
        client = ImmediateLogClient(TracingConfig(
            mode=LoggingMode.INSTANT,
            api_key=os.getenv("SYNTH_API_KEY"),
            base_url=os.getenv("SYNTH_ENDPOINT_OVERRIDE", "https://agent-learning.onrender.com")
        ))
        client.send_event(current_event, system_info)
```

This approach:
1. Maintains the same event/compute step relationship
2. Provides immediate streaming of complete events
3. Falls back to event_store if streaming fails
4. Requires minimal changes to existing code
5. Keeps the data structure consistent between immediate and deferred logging




RETRIES and RESILIENCE

Immediate Logging via a Dedicated Endpoint
Provide an endpoint (e.g. POST /v1/uploads/stream) that accepts a single Event object.
Add a small HTTP client (ImmediateLogClient) that sends just-completed events to that endpoint.
Same Data Structure
Continue using the same event/compute-step data structures. After the function completes, we package the event object with its compute steps and system information, and send it via the ImmediateLogClient.
Fallback to Event Store
If the streaming call fails (due to timeouts or connectivity issues), immediately store the event in the fallback “deferred” event store. This ensures no data is lost.
Optional Retry Queue
When immediate logging fails, add the event to an in-memory retry queue.
A background thread (for sync) or background task (for async) periodically attempts to replay these events to the streaming endpoint.

async
Concurrency and Context Management
For synchronous code, we store system info (system_name, system_id, system_instance_id) in thread-local variables (local).
For async code, use contextvars to store these same fields. Each async task can maintain its own system instance info without collisions.
The decorator or instrumentation code checks whether we’re in sync or async mode. Then it grabs the correct system info (from thread locals for sync, from contextvars for async) before calling ImmediateLogClient.
This ensures two concurrent async tasks don’t overwrite each other’s system info.
ASYNC Case
In the async decorators, retrieve system info from contextvars. For example:
         "system_instance_id": system_instance_id_var.get()
}
Pass this system_info to the ImmediateLogClient for streaming.
If streaming fails, fallback to event_store or queue for retry.
Preparing for Production
Under real load, measure overhead from immediate network calls. If overhead is high, consider batching events or sending only “critical” events immediately.
Evaluate concurrency concerns: ensure your background retry or fallback logic handles parallel requests properly, possibly using locks or concurrency-safe data structures.
By combining immediate streaming with concurrency safety, fallback to the event store, and an optional retry queue, you can maintain real-time logging for most events while guaranteeing that no data is lost if the network is down.


Finer details

I'll help update the plan to address these key concerns. Here's the improved implementation:

1. First, let's create a robust client configuration:

````python:synth_sdk/tracing/config.py
from enum import Enum
from pydantic import BaseModel
import os
from typing import Optional

class LoggingMode(Enum):
    INSTANT = "instant"
    DEFERRED = "deferred"

class TracingConfig(BaseModel):
    mode: LoggingMode
    api_key: str
    base_url: str = "https://agent-learning.onrender.com"
    max_retries: int = 3
    retry_backoff: float = 1.5  # exponential backoff multiplier
    batch_size: int = 1  # for future batching support
    timeout: float = 5.0  # seconds
````

2. Create a singleton client manager to handle both sync and async clients:

````python:synth_sdk/tracing/client_manager.py
import asyncio
import httpx
import requests
from typing import Optional
import random
import time
from functools import lru_cache
import logging

logger = logging.getLogger(__name__)

class ClientManager:
    _instance = None
    _sync_client = None
    _async_client = None
    _config = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    @classmethod
    def initialize(cls, config: TracingConfig):
        if cls._instance is None:
            cls._instance = cls()
        cls._instance._config = config
        return cls._instance

    @property
    def sync_client(self) -> requests.Session:
        if self._sync_client is None:
            self._sync_client = requests.Session()
            self._sync_client.headers["Authorization"] = f"Bearer {self._config.api_key}"
            adapter = requests.adapters.HTTPAdapter(
                pool_connections=100,
                pool_maxsize=100,
                max_retries=0,  # We handle retries ourselves
                pool_block=True
            )
            self._sync_client.mount("https://", adapter)
        return self._sync_client

    @property
    def async_client(self) -> httpx.AsyncClient:
        if self._async_client is None:
            self._async_client = httpx.AsyncClient(
                timeout=self._config.timeout,
                headers={"Authorization": f"Bearer {self._config.api_key}"},
                limits=httpx.Limits(max_connections=100, max_keepalive_connections=20)
            )
        return self._async_client

    def calculate_backoff(self, attempt: int) -> float:
        """Calculate exponential backoff with jitter"""
        base_delay = min(self._config.retry_backoff ** attempt, 60)  # cap at 60s
        jitter = random.uniform(0, 0.1 * base_delay)  # 10% jitter
        return base_delay + jitter
````

3. Update the immediate logging clients to use the manager:

````python:synth_sdk/tracing/immediate_client.py
from typing import Dict
import logging
from synth_sdk.tracing.abstractions import Event
from synth_sdk.tracing.config import TracingConfig
from synth_sdk.tracing.client_manager import ClientManager
from synth_sdk.tracing.events.store import event_store

logger = logging.getLogger(__name__)

class ImmediateLogClient:
    def __init__(self, config: TracingConfig):
        self.config = config
        self.client_manager = ClientManager.initialize(config)
    
    def send_event(self, event: Event, system_info: Dict[str, str]) -> bool:
        """Sends a single event with retries and fallback"""
        url = f"{self.config.base_url}/v1/uploads/stream"
        payload = {
            "event": event.to_dict(),
            "system_info": system_info
        }
        
        for attempt in range(self.config.max_retries):
            try:
                response = self.client_manager.sync_client.post(
                    url, 
                    json=payload,
                    timeout=self.config.timeout
                )
                response.raise_for_status()
                return True
            except Exception as e:
                backoff = self.client_manager.calculate_backoff(attempt)
                logger.warning(f"Attempt {attempt + 1} failed: {e}. Retrying in {backoff:.2f}s")
                time.sleep(backoff)
                
        logger.error("All retry attempts failed, falling back to event_store")
        event_store.add_event(
            system_info["system_name"],
            system_info["system_id"], 
            system_info["system_instance_id"],
            event
        )
        return False

class AsyncImmediateLogClient:
    def __init__(self, config: TracingConfig):
        self.config = config
        self.client_manager = ClientManager.initialize(config)
    
    async def send_event(self, event: Event, system_info: Dict[str, str]) -> bool:
        """Sends a single event with retries and fallback (async version)"""
        url = f"{self.config.base_url}/v1/uploads/stream"
        payload = {
            "event": event.to_dict(),
            "system_info": system_info
        }
        
        for attempt in range(self.config.max_retries):
            try:
                response = await self.client_manager.async_client.post(
                    url, 
                    json=payload,
                    timeout=self.config.timeout
                )
                response.raise_for_status()
                return True
            except Exception as e:
                backoff = self.client_manager.calculate_backoff(attempt)
                logger.warning(f"Attempt {attempt + 1} failed: {e}. Retrying in {backoff:.2f}s")
                await asyncio.sleep(backoff)
                
        logger.error("All retry attempts failed, falling back to event_store")
        event_store.add_event(
            system_info["system_name"],
            system_info["system_id"], 
            system_info["system_instance_id"],
            event
        )
        return False
````

4. Update the decorator to use the appropriate client:

````python:synth_sdk/tracing/decorators.py
# ... existing code ...

# Cache the config to avoid repeated env lookups
@lru_cache(maxsize=1)
def get_tracing_config() -> TracingConfig:
    return TracingConfig(
        mode=LoggingMode.INSTANT,
        api_key=os.getenv("SYNTH_API_KEY"),
        base_url=os.getenv("SYNTH_ENDPOINT_OVERRIDE", "https://agent-learning.onrender.com")
    )

# In trace_system_sync decorator
if logging_mode == LoggingMode.INSTANT:
    system_info = {
        "system_name": _local.system_name,
        "system_id": _local.system_id,
        "system_instance_id": _local.system_instance_id
    }
    client = ImmediateLogClient(get_tracing_config())
    client.send_event(current_event, system_info)

# In trace_system_async decorator
if logging_mode == LoggingMode.INSTANT:
    system_info = {
        "system_name": _local.system_name,
        "system_id": _local.system_id,
        "system_instance_id": _local.system_instance_id
    }
    client = AsyncImmediateLogClient(get_tracing_config())
    await client.send_event(current_event, system_info)
````

This improved implementation provides:

1. **Connection Pooling**: Using a singleton ClientManager with configured connection pools
2. **Cached Config**: Config and clients are created once and reused
3. **Proper Async Support**: Separate sync/async clients using appropriate libraries
4. **Robust Error Handling**: 
   - Exponential backoff with jitter
   - Multiple retry attempts
   - Graceful fallback to event_store
5. **Resource Management**: 
   - Proper timeout handling
   - Connection pooling limits
   - Reused HTTP clients

The next steps could include:
1. Adding metrics/telemetry
2. Implementing circuit breaker pattern
3. Adding support for batch sending
4. Adding more configuration options (timeouts, pool sizes, etc.)

Would you like me to elaborate on any of these aspects?




CONCRETE STEP BY STEP
# Implementation Steps for Production-Ready Immediate Logging

## Step 1: Base Configuration
- Create new `TracingConfig` class in `config.py` with:
  - LoggingMode enum (INSTANT/DEFERRED)
  - Connection settings (pool size, timeouts)
  - Retry settings (max retries, backoff)
  - API credentials
  - Base URL

## Step 2: Client Manager Singleton
- Create `ClientManager` singleton in `client_manager.py`
- Add connection pooling for both sync/async
- Add backoff calculation with jitter
- Cache API credentials and config

## Step 3: HTTP Client Implementation
- Add sync client using `requests.Session`
- Add async client using `httpx.AsyncClient` 
- Configure proper SSL/TLS settings
- Set up connection pooling limits

## Step 4: Base Log Client Interface
- Create abstract base class for logging clients
- Define common payload preparation
- Add retry logic with backoff
- Add fallback to event store

## Step 5: Sync/Async Client Implementation
- Create `ImmediateLogClient` for sync operations
- Create `AsyncImmediateLogClient` for async
- Add proper error handling
- Implement graceful fallback

## Step 6: Decorator Updates
- Update `trace_system_sync` decorator
- Update `trace_system_async` decorator
- Cache config lookups
- Add proper context management

## Step 7: Context Management
- Update thread local storage usage
- Add proper async context vars
- Ensure cleanup on errors
- Handle nested decorators

## Step 8: Error Handling & Resilience
- Add logic where if a streaming endpoint hit fails, we save it in memory and then next time try to send both
- reasonable (3s) retries for sending everything at the end of the run (batched upload)

## Step 9: Metrics & Monitoring
- Add success/failure counters
- Add latency histograms
- Add queue size monitoring
- Add error rate tracking

## Step 10: Testing Infrastructure
- Add unit tests for clients
- Add integration tests
- Add load tests
- Add failure scenario tests

Would you like me to proceed with implementing any of these steps in detail?
